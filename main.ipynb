{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO+D5lMj2FK+SEVhgUujxJT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4AZIGXbgDa7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDMeKf0yLxAP",
        "outputId": "28786c51-845d-4e12-d582-b31305d847ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tables in /usr/local/lib/python3.11/dist-packages (3.10.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from tables) (2.0.2)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.11/dist-packages (from tables) (2.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tables) (25.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from tables) (9.0.0)\n",
            "Requirement already satisfied: blosc2>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from tables) (3.3.4)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from tables) (4.14.0)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables) (1.10.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables) (1.1.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables) (4.3.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "uids = np.arange(1, 2001)\n",
        "periods = pd.period_range(start=\"2018-01\", end=\"2021-12\", freq=\"M\")\n",
        "times = pd.date_range(\"09:30\", \"16:00\", freq=\"5min\").time\n",
        "seconds = np.array([t.hour*3600 + t.minute*60 for t in times])\n",
        "monthly_dfs = []\n",
        "\n",
        "for p in periods:\n",
        "    days = pd.date_range(start=p.start_time.normalize(), periods=21, freq=\"D\")\n",
        "    all_dates = np.repeat(days.values, len(times))\n",
        "    all_seconds = np.tile(seconds, len(days))\n",
        "    dt_index = pd.to_datetime(all_dates.astype(\"datetime64[D]\")) + pd.to_timedelta(all_seconds, unit=\"s\")\n",
        "    midx = pd.MultiIndex.from_product([dt_index, uids], names=[\"DateTime\", \"UID\"])\n",
        "    data = np.random.randn(len(midx), 10)\n",
        "    cols = [f\"F{i}\" for i in range(1, 10)] + [\"T\"]\n",
        "    df = pd.DataFrame(data, index=midx, columns=cols)\n",
        "    monthly_dfs.append(df)"
      ],
      "metadata": {
        "id": "BhYe_GJKDm_H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assume `monthly_dfs` is your list of 36 DataFrames in order Jan 2018â€¦Dec 2021\n",
        "for df in monthly_dfs:\n",
        "    month = df.index.levels[0][0].strftime(\"%Y-%m\")  # e.g. \"2018-01\"\n",
        "    df.to_hdf(f\"data_{month}.h5\",\n",
        "              key=\"data\",\n",
        "              mode=\"w\",\n",
        "              format=\"table\",\n",
        "              data_columns=[\"UID\"])   # allows fast groupby on UID"
      ],
      "metadata": {
        "id": "lsqKhmr6LoOg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Imports & Hyperparams\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "# Adjust paths & training params here\n",
        "DATA_DIR    = \"/content\"        # where your data_YYYY-MM.h5 live\n",
        "WINDOW_SIZE = 78 * 5                 # e.g. 5-tick lookback (modify as needed)\n",
        "STEP        = 1\n",
        "BATCH_SIZE  = 256\n",
        "NUM_WORKERS = 4\n",
        "LR          = 1e-3\n",
        "EPOCHS      = 3\n",
        "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "OwhGGIlwLULI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Dataset: streams each HDF5 shard, one UID at a time, sliding windows\n",
        "class IntradayDataset(IterableDataset):\n",
        "    def __init__(self, h5_paths, window_size, step=1):\n",
        "        self.h5_paths    = h5_paths\n",
        "        self.window_size = window_size\n",
        "        self.step        = step\n",
        "\n",
        "    def __iter__(self):\n",
        "        worker = torch.utils.data.get_worker_info()\n",
        "        wid, n_w = (worker.id, worker.num_workers) if worker else (0,1)\n",
        "\n",
        "        # simple sharding of files across workers\n",
        "        my_files = [\n",
        "            p for idx, p in enumerate(self.h5_paths)\n",
        "            if idx % n_w == wid\n",
        "        ]\n",
        "\n",
        "        for path in my_files:\n",
        "            store = pd.HDFStore(path, mode=\"r\")\n",
        "            df    = store[\"data\"]  # your MultiIndex DF\n",
        "\n",
        "            # group by UID (inner index level)\n",
        "            for uid, sub in df.groupby(level=1):\n",
        "                arr = sub.values   # shape (Timestamps, 10)\n",
        "\n",
        "                # slide windows\n",
        "                for start in range(0, arr.shape[0] - self.window_size + 1, self.step):\n",
        "                    x = arr[start:start+self.window_size, :9]\n",
        "                    y = arr[start+self.window_size-1, 9]\n",
        "                    yield torch.from_numpy(x.astype(\"float32\")), torch.tensor(float(y), dtype=torch.float32)\n",
        "\n",
        "            store.close()"
      ],
      "metadata": {
        "id": "_Z9QzuMnLVbn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) DataLoader & Dataset instantiation\n",
        "h5_paths = sorted(glob.glob(f\"{DATA_DIR}/data_20*.h5\"))\n",
        "dataset  = IntradayDataset(h5_paths, window_size=WINDOW_SIZE, step=STEP)\n",
        "loader   = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2,\n",
        ")"
      ],
      "metadata": {
        "id": "NPrf-XHQLZxn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h5_paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKqL2QepPV24",
        "outputId": "6004348a-245b-42ff-8f64-d1c5213d21a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/data_2018-01.h5',\n",
              " '/content/data_2018-02.h5',\n",
              " '/content/data_2018-03.h5',\n",
              " '/content/data_2018-04.h5',\n",
              " '/content/data_2018-05.h5',\n",
              " '/content/data_2018-06.h5',\n",
              " '/content/data_2018-07.h5',\n",
              " '/content/data_2018-08.h5',\n",
              " '/content/data_2018-09.h5',\n",
              " '/content/data_2018-10.h5',\n",
              " '/content/data_2018-11.h5',\n",
              " '/content/data_2018-12.h5',\n",
              " '/content/data_2019-01.h5',\n",
              " '/content/data_2019-02.h5',\n",
              " '/content/data_2019-03.h5',\n",
              " '/content/data_2019-04.h5',\n",
              " '/content/data_2019-05.h5',\n",
              " '/content/data_2019-06.h5',\n",
              " '/content/data_2019-07.h5',\n",
              " '/content/data_2019-08.h5',\n",
              " '/content/data_2019-09.h5',\n",
              " '/content/data_2019-10.h5',\n",
              " '/content/data_2019-11.h5',\n",
              " '/content/data_2019-12.h5',\n",
              " '/content/data_2020-01.h5',\n",
              " '/content/data_2020-02.h5',\n",
              " '/content/data_2020-03.h5',\n",
              " '/content/data_2020-04.h5',\n",
              " '/content/data_2020-05.h5',\n",
              " '/content/data_2020-06.h5',\n",
              " '/content/data_2020-07.h5',\n",
              " '/content/data_2020-08.h5',\n",
              " '/content/data_2020-09.h5',\n",
              " '/content/data_2020-10.h5',\n",
              " '/content/data_2020-11.h5',\n",
              " '/content/data_2020-12.h5',\n",
              " '/content/data_2021-01.h5',\n",
              " '/content/data_2021-02.h5',\n",
              " '/content/data_2021-03.h5',\n",
              " '/content/data_2021-04.h5',\n",
              " '/content/data_2021-05.h5',\n",
              " '/content/data_2021-06.h5',\n",
              " '/content/data_2021-07.h5',\n",
              " '/content/data_2021-08.h5',\n",
              " '/content/data_2021-09.h5',\n",
              " '/content/data_2021-10.h5',\n",
              " '/content/data_2021-11.h5',\n",
              " '/content/data_2021-12.h5']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) LSTM model definition\n",
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_size=9, hidden_size=128, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size,\n",
        "                            hidden_size,\n",
        "                            num_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout)\n",
        "        self.fc   = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len, features]\n",
        "        out, _ = self.lstm(x)       # out: [batch, seq_len, hidden]\n",
        "        last   = out[:, -1, :]      # take last time step\n",
        "        return self.fc(last).squeeze(1)  # [batch]"
      ],
      "metadata": {
        "id": "2cfRnqu1LbpT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Training loop\n",
        "model = LSTMRegressor().to(DEVICE)\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for i, (x, y) in enumerate(loader, 1):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch {epoch} | Batch {i} | Avg Loss {(total_loss/i):.6f}\")\n",
        "\n",
        "    print(f\"End Epoch {epoch} | Avg Loss {total_loss/i:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "vVh1nfKDLelB",
        "outputId": "83e3b63d-854f-4eb7-fe2b-3b1a62b763e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Batch 50 | Avg Loss 1.008062\n",
            "Epoch 1 | Batch 100 | Avg Loss 1.003806\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0d96352c1697>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Save your model\n",
        "torch.save(model.state_dict(), \"lstm_regressor.pth\")\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "tfHrtKtgLgdd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}